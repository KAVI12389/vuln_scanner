import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def check(url):
    try:
        if not url.endswith("/"):
            url += "/"

        # --- Step 1: Request page ---
        resp = requests.get(
            url,
            timeout=15,
            allow_redirects=True,
            headers={"User-Agent": "AdvancedScanner/2.0"}
        )
        resp.raise_for_status()
        content = resp.text.lower()

        # --- Step 2: Directory listing signatures ---
        indicators = [
            "index of", "parent directory", "last modified",
            "directory listing", "apache server at", "nginx autoindex",
            "tomcat directory", "iis7", "generated by h5ai"
        ]
        keyword_found = any(word in content for word in indicators)

        # --- Step 3: Extract file/folder links ---
        soup = BeautifulSoup(resp.text, "html.parser")
        links = []
        for a in soup.find_all("a", href=True):
            href = a.get("href")
            if not href:
                continue
            if href.lower() in ["../", "/", "#"]:
                continue
            # Convert relative â†’ absolute path
            abs_url = urljoin(url, href)
            links.append(abs_url)

        suspicious = list(set(links))  # unique

        # --- Step 4: Decision ---
        if keyword_found and suspicious:
            preview = ", ".join(suspicious[:15])  # show first 15
            if len(suspicious) > 15:
                preview += " ..."
            return (
                f"[!!] Directory Listing Enabled at {url}\n"
                f"     Open Files/Folders ({len(suspicious)}): {preview}"
            )

        elif keyword_found:
            return f"[!!] Possible Directory Listing Detected at {url} (no extractable files)"

        else:
            return f"[+] No open directory listing at {url}"

    # --- Step 5: Advanced error handling ---
    except requests.exceptions.Timeout:
        return f"[ERROR] Timeout while connecting to {url} (server took too long)"
    except requests.exceptions.ConnectionError:
        return f"[ERROR] Connection failed for {url} (server unreachable)"
    except requests.exceptions.HTTPError as e:
        return f"[ERROR] HTTP error {e.response.status_code} at {url}"
    except Exception as e:
        return f"[ERROR] Unexpected error at {url}: {str(e)}"



